{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string\n",
    "hacker_string = \"A computer hacker is any skilled computer expert that uses their technical knowledge to overcome a problem. While \\\"hacker\\\" can refer to any skilled computer programmer, the term has become associated in popular culture with a \\\"security hacker\\\", someone who, with their technical knowledge, uses bugs or exploits to break into computer systems.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'computer',\n",
       " 'hacker',\n",
       " 'is',\n",
       " 'any',\n",
       " 'skilled',\n",
       " 'computer',\n",
       " 'expert',\n",
       " 'that',\n",
       " 'uses',\n",
       " 'their',\n",
       " 'technical',\n",
       " 'knowledge',\n",
       " 'to',\n",
       " 'overcome',\n",
       " 'a',\n",
       " 'problem',\n",
       " '.',\n",
       " 'While',\n",
       " '``',\n",
       " 'hacker',\n",
       " \"''\",\n",
       " 'can',\n",
       " 'refer',\n",
       " 'to',\n",
       " 'any',\n",
       " 'skilled',\n",
       " 'computer',\n",
       " 'programmer',\n",
       " ',',\n",
       " 'the',\n",
       " 'term',\n",
       " 'has',\n",
       " 'become',\n",
       " 'associated',\n",
       " 'in',\n",
       " 'popular',\n",
       " 'culture',\n",
       " 'with',\n",
       " 'a',\n",
       " '``',\n",
       " 'security',\n",
       " 'hacker',\n",
       " \"''\",\n",
       " ',',\n",
       " 'someone',\n",
       " 'who',\n",
       " ',',\n",
       " 'with',\n",
       " 'their',\n",
       " 'technical',\n",
       " 'knowledge',\n",
       " ',',\n",
       " 'uses',\n",
       " 'bugs',\n",
       " 'or',\n",
       " 'exploits',\n",
       " 'to',\n",
       " 'break',\n",
       " 'into',\n",
       " 'computer',\n",
       " 'systems',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing\n",
    "hacker_string_tokens = word_tokenize(hacker_string)\n",
    "hacker_string_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 63)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the type and number of tokens\n",
    "type(hacker_string_tokens), len(hacker_string_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency of tokens\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'computer': 4, ',': 4, 'hacker': 3, 'to': 3, 'any': 2, 'skilled': 2, 'uses': 2, 'their': 2, 'technical': 2, 'knowledge': 2, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in hacker_string_tokens:\n",
    "    fdist[i] = fdist[i] + 1\n",
    "\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computer', 4),\n",
       " (',', 4),\n",
       " ('hacker', 3),\n",
       " ('to', 3),\n",
       " ('any', 2),\n",
       " ('skilled', 2),\n",
       " ('uses', 2),\n",
       " ('their', 2),\n",
       " ('technical', 2),\n",
       " ('knowledge', 2)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ten most common tokens\n",
    "top_10 = fdist.most_common(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigrams, ngram\n",
    "black_smoke = \"Did you know there was a tower, Where they look out to the land, To see the people quickly passing by\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Did',\n",
       " 'you',\n",
       " 'know',\n",
       " 'there',\n",
       " 'was',\n",
       " 'a',\n",
       " 'tower',\n",
       " ',',\n",
       " 'Where',\n",
       " 'they',\n",
       " 'look',\n",
       " 'out',\n",
       " 'to',\n",
       " 'the',\n",
       " 'land',\n",
       " ',',\n",
       " 'To',\n",
       " 'see',\n",
       " 'the',\n",
       " 'people',\n",
       " 'quickly',\n",
       " 'passing',\n",
       " 'by']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "black_smoke_tokens = word_tokenize(black_smoke)\n",
    "black_smoke_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you'),\n",
       " ('you', 'know'),\n",
       " ('know', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'tower'),\n",
       " ('tower', ','),\n",
       " (',', 'Where'),\n",
       " ('Where', 'they'),\n",
       " ('they', 'look'),\n",
       " ('look', 'out'),\n",
       " ('out', 'to'),\n",
       " ('to', 'the'),\n",
       " ('the', 'land'),\n",
       " ('land', ','),\n",
       " (',', 'To'),\n",
       " ('To', 'see'),\n",
       " ('see', 'the'),\n",
       " ('the', 'people'),\n",
       " ('people', 'quickly'),\n",
       " ('quickly', 'passing'),\n",
       " ('passing', 'by')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_list = list(nltk.bigrams(black_smoke_tokens))\n",
    "bigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you', 'know'),\n",
       " ('you', 'know', 'there'),\n",
       " ('know', 'there', 'was'),\n",
       " ('there', 'was', 'a'),\n",
       " ('was', 'a', 'tower'),\n",
       " ('a', 'tower', ','),\n",
       " ('tower', ',', 'Where'),\n",
       " (',', 'Where', 'they'),\n",
       " ('Where', 'they', 'look'),\n",
       " ('they', 'look', 'out'),\n",
       " ('look', 'out', 'to'),\n",
       " ('out', 'to', 'the'),\n",
       " ('to', 'the', 'land'),\n",
       " ('the', 'land', ','),\n",
       " ('land', ',', 'To'),\n",
       " (',', 'To', 'see'),\n",
       " ('To', 'see', 'the'),\n",
       " ('see', 'the', 'people'),\n",
       " ('the', 'people', 'quickly'),\n",
       " ('people', 'quickly', 'passing'),\n",
       " ('quickly', 'passing', 'by')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_list = list(nltk.trigrams(black_smoke_tokens))\n",
    "trigrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Did', 'you', 'know', 'there'),\n",
       " ('you', 'know', 'there', 'was'),\n",
       " ('know', 'there', 'was', 'a'),\n",
       " ('there', 'was', 'a', 'tower'),\n",
       " ('was', 'a', 'tower', ','),\n",
       " ('a', 'tower', ',', 'Where'),\n",
       " ('tower', ',', 'Where', 'they'),\n",
       " (',', 'Where', 'they', 'look'),\n",
       " ('Where', 'they', 'look', 'out'),\n",
       " ('they', 'look', 'out', 'to'),\n",
       " ('look', 'out', 'to', 'the'),\n",
       " ('out', 'to', 'the', 'land'),\n",
       " ('to', 'the', 'land', ','),\n",
       " ('the', 'land', ',', 'To'),\n",
       " ('land', ',', 'To', 'see'),\n",
       " (',', 'To', 'see', 'the'),\n",
       " ('To', 'see', 'the', 'people'),\n",
       " ('see', 'the', 'people', 'quickly'),\n",
       " ('the', 'people', 'quickly', 'passing'),\n",
       " ('people', 'quickly', 'passing', 'by')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_list = list(nltk.ngrams(black_smoke_tokens, 4))\n",
    "ngrams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('win', 'studi', 'buy')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"winning\"), pst.stem(\"studies\"), pst.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "from nltk.stem import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_to_stem = [\"cats\", \"cacti\", \"geese\", \"dog\", \"Buy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cats : cat\n",
      "cacti : cactus\n",
      "geese : goose\n",
      "dog : dog\n",
      "Buy : Buy\n"
     ]
    }
   ],
   "source": [
    "for i in words_to_stem:\n",
    "    print(i + \" : \" + lemmatizer.lemmatize(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoS\n",
    "peace = \"What do you mean, 'I don't believe in God'? I talk to him everyday.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "peace_tokens = word_tokenize(peace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP')]\n",
      "[('do', 'VB')]\n",
      "[('you', 'PRP')]\n",
      "[('mean', 'NN')]\n",
      "[(',', ',')]\n",
      "[(\"'\", \"''\")]\n",
      "[('I', 'PRP')]\n",
      "[('do', 'VB')]\n",
      "[(\"n't\", 'RB')]\n",
      "[('believe', 'VB')]\n",
      "[('in', 'IN')]\n",
      "[('God', 'NNP')]\n",
      "[(\"'\", \"''\")]\n",
      "[('?', '.')]\n",
      "[('I', 'PRP')]\n",
      "[('talk', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('him', 'PRP')]\n",
      "[('everyday', 'NN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in peace_tokens:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mary = \"Mary had a little lamb, whom she really loved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mary_tokens = word_tokenize(mary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mary', 'NNP')]\n",
      "[('had', 'VBD')]\n",
      "[('a', 'DT')]\n",
      "[('little', 'JJ')]\n",
      "[('lamb', 'NN')]\n",
      "[(',', ',')]\n",
      "[('whom', 'WP')]\n",
      "[('she', 'PRP')]\n",
      "[('really', 'RB')]\n",
      "[('loved', 'VBN')]\n"
     ]
    }
   ],
   "source": [
    "for i in mary_tokens:\n",
    "    print(nltk.pos_tag([i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Names Entity Recognition\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
